<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.7.4">Jekyll</generator>
  <link href="http://localhost:4000/tag/object-detection/feed.xml" rel="self" type="application/atom+xml" />
  <link href="http://localhost:4000/" rel="alternate" type="text/html" />
  <updated>2019-09-16T16:59:58-07:00</updated>
  <id>http://localhost:4000/tag/object-detection/feed.xml</id>

  
  
  

  
    <title type="html">Parag Mali | </title>
  

  
    <subtitle>Parag Mali</subtitle>
  

  

  
    
      
    
      
    
  

  
  

  
    <entry>
      <title type="html">Bounding Box Regression</title>
      <link href="http://localhost:4000/bounding-box-regression" rel="alternate" type="text/html" title="Bounding Box Regression" />
      <published>2019-09-15T21:16:00-07:00</published>
      <updated>2019-09-15T21:16:00-07:00</updated>
      <id>http://localhost:4000/bounding-box-regression</id>
      <content type="html" xml:base="http://localhost:4000/bounding-box-regression">&lt;!DOCTYPE html&gt;
&lt;html xmlns=&quot;http://www.w3.org/1999/xhtml&quot; lang=&quot;&quot; xml:lang=&quot;&quot;&gt;
&lt;head&gt;
  &lt;meta charset=&quot;utf-8&quot; /&gt;
  &lt;meta name=&quot;generator&quot; content=&quot;pandoc&quot; /&gt;
  &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1.0, user-scalable=yes&quot; /&gt;
  &lt;title&gt;This post explains the loss function used in the SSD object detection.&lt;/title&gt;
  &lt;style&gt;
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  &lt;/style&gt;
  &lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;
  &lt;!--[if lt IE 9]&gt;
    &lt;script src=&quot;//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js&quot;&gt;&lt;/script&gt;
  &lt;![endif]--&gt;
&lt;/head&gt;
&lt;body&gt;

&lt;p&gt;In this post, let us understand the loss function used by Liu et al. in the SSD paper &lt;span class=&quot;citation&quot; data-cites=&quot;liu2016ssd&quot;&gt;(Liu et al. 2016)&lt;/span&gt;. Let &lt;span class=&quot;math inline&quot;&gt;\(x_{ij}^{p} = \{1, 0\}\)&lt;/span&gt; be an indicator for matching the &lt;span class=&quot;math inline&quot;&gt;\(i^{th}\)&lt;/span&gt; default box to the &lt;span class=&quot;math inline&quot;&gt;\(j^{th}\)&lt;/span&gt; ground truth box of category &lt;span class=&quot;math inline&quot;&gt;\(p\)&lt;/span&gt;. In the matching strategy used in the model, we can have &lt;span class=&quot;math inline&quot;&gt;\(\sum_{i}x_{ij}^{p} \ge 1\)&lt;/span&gt;. It means that more than one default box can match one ground truth box. The overall loss is a weighted sum of localization loss and confidence loss. Next, we describe the bounding box regression problem and the localization loss.&lt;/p&gt;

&lt;amp-auto-ads type=&quot;adsense&quot; data-ad-client=&quot;ca-pub-7615441328920742&quot;&gt;
&lt;/amp-auto-ads&gt;

&lt;h2 id=&quot;bounding-box-regression&quot;&gt;Bounding Box Regression&lt;/h2&gt;
&lt;p&gt;Let us define the predicted box &lt;span class=&quot;math inline&quot;&gt;\(l\)&lt;/span&gt; as &lt;span class=&quot;math inline&quot;&gt;\(l = \{l^{cx}, l^{cy}, l^{w}, l^{h}\}\)&lt;/span&gt;, where superscript (&lt;span class=&quot;math inline&quot;&gt;\(cx\)&lt;/span&gt;, &lt;span class=&quot;math inline&quot;&gt;\(cy\)&lt;/span&gt;) is the x and y co-ordinate of the center of the box, &lt;span class=&quot;math inline&quot;&gt;\(w\)&lt;/span&gt; is the width, and, &lt;span class=&quot;math inline&quot;&gt;\(h\)&lt;/span&gt; is the height. Similarly, the ground truth bounding box &lt;span class=&quot;math inline&quot;&gt;\(g\)&lt;/span&gt; can be defined as &lt;span class=&quot;math inline&quot;&gt;\(l = \{l^{cx}, l^{cy}, l^{w}, l^{h}\}\)&lt;/span&gt;. We configure the bounding box regressor to learn four functions, &lt;span class=&quot;math inline&quot;&gt;\(f_{cx}, f_{cy}\)&lt;/span&gt; for scale-invariant transformation between centers of &lt;span class=&quot;math inline&quot;&gt;\(l\)&lt;/span&gt; and &lt;span class=&quot;math inline&quot;&gt;\(g\)&lt;/span&gt; and &lt;span class=&quot;math inline&quot;&gt;\(f_w, f_h\)&lt;/span&gt; for log-scale transformation between widths and heights. In general, we define &lt;span class=&quot;math inline&quot;&gt;\(f_i\)&lt;/span&gt;, where &lt;span class=&quot;math inline&quot;&gt;\(i \in \{cx,cy,w,h\}\)&lt;/span&gt; as a bounding box correction function. &lt;span class=&quot;math inline&quot;&gt;\(f_i\)&lt;/span&gt; takes &lt;span class=&quot;math inline&quot;&gt;\(l\)&lt;/span&gt; as the input. The target values &lt;span class=&quot;math inline&quot;&gt;\(\hat{g}\)&lt;/span&gt; for bounding box correction functions &lt;span class=&quot;math inline&quot;&gt;\(f_i\)&lt;/span&gt; are defined below:&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[\begin{split}
                \hat{g}^{cx}_j &amp;amp;= (g^{cx}_j - d^{cx}_i)/d_i^{w}\\
                \hat{g}^{cy}_j &amp;amp;= (g^{cy}_j - d^{cy}_i)/d_i^{h}\\
                \hat{g}^w_j &amp;amp;= \log(\frac{g^w_j}{d^w_i})\\
                \hat{g}^h_j &amp;amp;= \log(\frac{g^h_j}{d^h_i})
            \end{split}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We use &lt;span class=&quot;math inline&quot;&gt;\(smooth_{L1}\)&lt;/span&gt; loss defined by Girshick&lt;span class=&quot;citation&quot; data-cites=&quot;girshick2015fast&quot;&gt;(Girshick 2015)&lt;/span&gt; for the bounding box regression problem. It is given by Equation &lt;a href=&quot;#eq:smooth_l1&quot; data-reference-type=&quot;ref&quot; data-reference=&quot;eq:smooth_l1&quot;&gt;[eq:smooth_l1]&lt;/a&gt;. It is claimed to be less sensitive to outliers.&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[smooth_{L1}(t) = 
            \begin{cases}
            0.5t^2,&amp;amp; \text{if} \mid{t}\mid&amp;lt;1;\\
            \mid{t}\mid-0.5,&amp;amp; \text{otherwise}
            \end{cases}
            \label{eq:smooth_l1}\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&quot;localization-loss&quot;&gt;Localization Loss&lt;/h2&gt;
&lt;p&gt;The localization loss is the smooth L1 loss &lt;span class=&quot;citation&quot; data-cites=&quot;ErhanSTA13&quot;&gt;(Erhan et al. 2013)&lt;/span&gt; between the predicted box(&lt;span class=&quot;math inline&quot;&gt;\(l\)&lt;/span&gt;) and the ground truth box(&lt;span class=&quot;math inline&quot;&gt;\(g\)&lt;/span&gt;). We calculate the overall localization loss as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[\begin{split}
                L_{loc}(x,l,g,f) &amp;amp;= \sum_{i \in POS} \text{ } \sum_{m \in \{cx, cy, w, h\}} (x^{p}_{ij} smooth_{L1}(f_m(l_{i})-\hat{g}^{m}_{j}))\\
             \end{split}
            \label{eq:localization_loss}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where POS are positive examples and &lt;span class=&quot;math inline&quot;&gt;\(\mid POS \mid = N\)&lt;/span&gt;. &lt;span class=&quot;math inline&quot;&gt;\(x\)&lt;/span&gt; is the indicator function, &lt;span class=&quot;math inline&quot;&gt;\(l\)&lt;/span&gt; is the predicted box, &lt;span class=&quot;math inline&quot;&gt;\(g\)&lt;/span&gt; is the ground truth, and, &lt;span class=&quot;math inline&quot;&gt;\(f_i\)&lt;/span&gt; is the set of bounding box correction functions. From Equation &lt;a href=&quot;#eq:localization_loss&quot; data-reference-type=&quot;ref&quot; data-reference=&quot;eq:localization_loss&quot;&gt;[eq:localization_loss]&lt;/a&gt; we can see that the localization loss is calculated only for positive examples.&lt;/p&gt;
&lt;h2 id=&quot;confidence-loss&quot;&gt;Confidence Loss&lt;/h2&gt;
&lt;p&gt;The confidence loss is the cross entropy loss over multiple class confidences &lt;span class=&quot;math inline&quot;&gt;\((c)\)&lt;/span&gt;. It is given by Equation &lt;a href=&quot;#eq:ce_confidence_loss&quot; data-reference-type=&quot;ref&quot; data-reference=&quot;eq:ce_confidence_loss&quot;&gt;[eq:ce_confidence_loss]&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[\begin{split}
                L_{conf}(x,c) &amp;amp;= - \sum_{i \in POS}{x_{ij}^p}\log{\hat{c}_{i}^{p}} - \sum_{i \in NEG} \log(\hat{c}_i^{0})\\ 
                \text{where,}\\ 
                \hat{c}_i^{p} &amp;amp;= exp(c_i^p) / \sum_k exp(c_i^k)
            \end{split}
            \label{eq:ce_confidence_loss}\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&quot;overall-loss&quot;&gt;Overall Loss&lt;/h2&gt;
&lt;p&gt;The overall objective loss function used in our model is given by Equation &lt;a href=&quot;#eq:overall_loss&quot; data-reference-type=&quot;ref&quot; data-reference=&quot;eq:overall_loss&quot;&gt;[eq:overall_loss]&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[\begin{split}
                L(x,c,l,g) = \frac{1}{N}(L_{conf}(x,c) + \alpha L_{loc}(x,l,g,f))
            \end{split}
        \label{eq:overall_loss}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where N is the number of matched default boxes. If N is 0, the loss is set to 0. We set the weight term &lt;span class=&quot;math inline&quot;&gt;\(\alpha\)&lt;/span&gt; to 1 like original SSD&lt;span class=&quot;citation&quot; data-cites=&quot;liu2016ssd&quot;&gt;(Liu et al. 2016)&lt;/span&gt;.&lt;/p&gt;

&lt;h2&gt;References&lt;/h2&gt;
    &lt;div id=&quot;ref-liu2016ssd&quot;&gt;
    &lt;p&gt;Liu, Wei, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, and Alexander C Berg. 2016. “Ssd: Single Shot Multibox Detector.” In &lt;em&gt;European Conference on Computer Vision&lt;/em&gt;, 21–37. Springer.&lt;/p&gt;
    &lt;/div&gt;
    &lt;div id=&quot;ref-girshick2015fast&quot;&gt;
    &lt;p&gt;Girshick, Ross. 2015. “Fast R-Cnn.” In &lt;em&gt;Proceedings of the Ieee International Conference on Computer Vision&lt;/em&gt;, 1440–8.&lt;/p&gt;
    &lt;/div&gt;
    &lt;div id=&quot;ref-ErhanSTA13&quot;&gt;
    &lt;p&gt;Erhan, Dumitru, Christian Szegedy, Alexander Toshev, and Dragomir Anguelov. 2013. “Scalable Object Detection Using Deep Neural Networks.” &lt;em&gt;CoRR&lt;/em&gt; abs/1312.2249. &lt;a href=&quot;http://arxiv.org/abs/1312.2249&quot;&gt;http://arxiv.org/abs/1312.2249&lt;/a&gt;.&lt;/p&gt;
    &lt;/div&gt;
&lt;/body&gt;</content>

      
      
      
      
      

      <author>
          <name>Parag Mali</name>
        
        
      </author>

      

      
        <category term="Object Detection" />
      

      
        <summary type="html">This post explains the loss function used in the SSD object detection.</summary>
      

      
      
    </entry>
  
</feed>
